# -*- coding: utf-8 -*-

[hyperparams]
word_dim = 300
state_dim = 512
aggregation = "sum"
attention = "nonlinear"
retrofitting = False
alpha = 0.0
scale = 0.01
identity_penalty = False
lambda = 0.0
grad_clip = 5.0
weight_decay = 4e-6
batch_size = 180
